{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7a5fc17",
   "metadata": {},
   "source": [
    "# ☁️ NimbusOps – Monitoring Notebook\n",
    "\n",
    "This notebook shows how to:\n",
    "\n",
    "1. Connect to the MLflow tracking backend used by NimbusOps  \n",
    "2. List experiments and runs  \n",
    "3. Inspect metrics for recent training runs  \n",
    "4. Visualize accuracy and AUC over time  \n",
    "\n",
    "By default, it assumes the MLflow tracking URI is the local `mlruns` directory\n",
    "in the project root (NimbusOps/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa01a313",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "import mlflow\n",
    "import mlflow.tracking\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PROJECT_ROOT = Path(__file__).resolve().parents[1]\n",
    "DEFAULT_TRACKING_URI = PROJECT_ROOT / \"mlruns\"\n",
    "\n",
    "tracking_uri = os.environ.get(\"MLFLOW_TRACKING_URI\", str(DEFAULT_TRACKING_URI))\n",
    "mlflow.set_tracking_uri(tracking_uri)\n",
    "\n",
    "print(\"Project root:\", PROJECT_ROOT)\n",
    "print(\"Using MLflow tracking URI:\", tracking_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b55bb7",
   "metadata": {},
   "source": [
    "## 1. List MLflow experiments\n",
    "\n",
    "NimbusOps training (`src/train.py`) logs to an experiment named\n",
    "`nimbusops-breast-cancer` by default.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7eae27",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = mlflow.tracking.MlflowClient()\n",
    "\n",
    "experiments = client.list_experiments()\n",
    "exp_df = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"name\": e.name,\n",
    "            \"experiment_id\": e.experiment_id,\n",
    "            \"artifact_location\": e.artifact_location,\n",
    "            \"lifecycle_stage\": e.lifecycle_stage,\n",
    "        }\n",
    "        for e in experiments\n",
    "    ]\n",
    ")\n",
    "\n",
    "exp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aab6586",
   "metadata": {},
   "source": [
    "## 2. Load runs for NimbusOps experiment\n",
    "\n",
    "We focus on the `nimbusops-breast-cancer` experiment and pull recent runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e98338e",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"nimbusops-breast-cancer\"\n",
    "experiment = client.get_experiment_by_name(experiment_name)\n",
    "\n",
    "if experiment is None:\n",
    "    raise RuntimeError(f\"Experiment '{experiment_name}' not found yet. \"\n",
    "                       \"Run src/train.py at least once.\")\n",
    "\n",
    "print(\"Using experiment:\", experiment.name, \"id:\", experiment.experiment_id)\n",
    "\n",
    "runs = client.search_runs(\n",
    "    experiment_ids=[experiment.experiment_id],\n",
    "    filter_string=\"\",\n",
    "    max_results=100,\n",
    "    order_by=[\"attributes.start_time DESC\"],\n",
    ")\n",
    "\n",
    "rows = []\n",
    "for r in runs:\n",
    "    row = {\n",
    "        \"run_id\": r.info.run_id,\n",
    "        \"start_time\": pd.to_datetime(r.info.start_time, unit=\"ms\"),\n",
    "    }\n",
    "    row.update({f\"param_{k}\": v for k, v in r.data.params.items()})\n",
    "    row.update({f\"metric_{k}\": v for k, v in r.data.metrics.items()})\n",
    "    rows.append(row)\n",
    "\n",
    "runs_df = pd.DataFrame(rows)\n",
    "runs_df.sort_values(\"start_time\", inplace=True)\n",
    "runs_df.reset_index(drop=True, inplace=True)\n",
    "runs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51762afa",
   "metadata": {},
   "source": [
    "## 3. Visualize metrics over time\n",
    "\n",
    "We plot accuracy and AUC for recent runs to see how the model is evolving.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcbbf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"default\")\n",
    "\n",
    "metric_cols = [c for c in runs_df.columns if c.startswith(\"metric_\")]\n",
    "if not metric_cols:\n",
    "    print(\"No metrics logged yet. Run src/train.py to create some runs.\")\n",
    "else:\n",
    "    fig, axes = plt.subplots(len(metric_cols), 1, figsize=(8, 4 * len(metric_cols)))\n",
    "    if len(metric_cols) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, col in zip(axes, metric_cols):\n",
    "        ax.plot(runs_df[\"start_time\"], runs_df[col], marker=\"o\")\n",
    "        ax.set_title(col.replace(\"metric_\", \"\").upper())\n",
    "        ax.set_xlabel(\"run start time\")\n",
    "        ax.set_ylabel(col.replace(\"metric_\", \"\"))\n",
    "        ax.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f09f7e8",
   "metadata": {},
   "source": [
    "## 4. Inspect a single run\n",
    "\n",
    "Pick the last run and inspect its parameters and metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4658a6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not runs_df.empty:\n",
    "    last_run_id = runs_df.iloc[-1][\"run_id\"]\n",
    "    last_run = client.get_run(last_run_id)\n",
    "\n",
    "    print(\"Run ID:\", last_run_id)\n",
    "    print(\"\\nParams:\")\n",
    "    display(pd.DataFrame(last_run.data.params.items(), columns=[\"param\", \"value\"]))\n",
    "\n",
    "    print(\"\\nMetrics:\")\n",
    "    display(pd.DataFrame(last_run.data.metrics.items(), columns=[\"metric\", \"value\"]))\n",
    "else:\n",
    "    print(\"No runs found yet.\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
